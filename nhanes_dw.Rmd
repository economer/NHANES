---
title: "Downloading All files in National Health and Nutrition Examination Survey (NHANES)"
author: "S.H.Hosseini"
date: "25/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. National Health and Nutrition Examination Survey (NHANES): Downloading and Joining Individual files.

The National Health and Nutrition Examination Survey (NHANES) includes a series of data related the health and nutritional status of adults and children in the United States. You can find more information about NHANES here: <https://www.cdc.gov/nchs/nhanes/about_nhanes.htm>

The data is available online and free of use. First, I show how to download and join a couple of datasets from the NHANES webpage. However, you can follow instructions at Section 2 for downloading and joining all the data available since 1999.

## 1.2. The pacakges required

To download and clean the individual data files we require haven and tidyverse. Just in case we need to work with date and time variables we can install and load lubridate and hms. I also prefer to clean the variables names so I load janitor to be able to use clean_names() function.

For the second task the primary packages required are glue, rvest, stringr and data.table.

```{r,echo=F}
library(haven)
library(tidyverse)
library(lubridate)
library(hms)
library(janitor)
library(glue)
library(rvest)
library(stringr)
library(data.table)
```

## 1.3. Download the files required

NHANES includes several datasets including demographic data, dietary data, examination data and laboratory data. Each dataset could also include several files. For instance demographic dataset includes only one file. However, the examination data contains information about blood pressure, body measures, oral health etc.

Here, we focus on few files including demographic, blood pressure, body measures, insulin and finally iron status (from laboratory datasets). These datasets were all collected in 2017-2018. Although, NHANES is not a longitudinal dataset, many studies tend to pool NAHNES data over years.

### 1.3.1. Download the demographic data

To download the demographic data one can simply use the online address of the file at:

[Files locations](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=2017)

,and then find the location of file under "Data File" that is in "xpt" format.

```{r}
demo_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT") %>%
  clean_names()

```

demo_2017 includes several variables that might be confusing to use without knowing the labels. However, it is possible to use setnames() function of data.table to change the variables' names to their labels. In Section 2, it will be shown how to perfom this taks.

### 1.3.2. Download blood pressure and body measures from Examination Data along with insulin and iron status from Laboratory Dataset.

The Examination Data for 2017-2018 can be found here: <https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=2017>

The Laboratory Data for 2017-2018 can be found here: <https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear=2017>

It should be noted that there will be several missing values in the final dataset. This is because not every person participated in the primary sample was eligible to participate in all of the examinations or laboratory tests conducted. For instance, in the case of insulin data, only individuals above 12 years of age were taken into consideration.

```{r}
# blood pressure data
blood_pressure_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/BPX_J.XPT") %>%
  clean_names()

## body measures data
body_measures_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/BMX_J.XPT") %>%
  clean_names()

## insulin data
insulin_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/INS_J.XPT") %>%
  clean_names()

## iron status 
iron_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/FETIB_J.XPT") %>%
  clean_names()
```

## 1.4. Joining the datasets

Each individual is assigned an ID stored in the seqn variable that can be used for joining the downloaded datasets. Here one can use left_join() function in dplyr however, it takes some times to include every dataset (example is provided below). So, it seems that join_all() function from plyr package is a less time consuming choice. Note that the type of JOIN in plyr::join_all() function should be chosen that is "left" in this case.

**Note**: As it is noted by plyr package documentation "loading plyr and dplyr packages in the same time is likely to cause problems.If you need functions from both plyr and dplyr, please load plyr first, then dplyr"

```{r}
library(plyr)
library(dplyr)
## using dplyr to join 3 datasts. 
nh_2017_dplyr <- left_join(x = demo_2017,y = blood_pressure_2017,by="seqn") %>%
              left_join(.,body_measures_2017,by="seqn")
    
## using join_all from plyr. 

nh_2017 <- plyr::join_all(dfs = list(demo_2017,body_measures_2017,blood_pressure_2017,iron_2017,insulin_2017),by = "seqn",type = "left")

```

# 2. Importing all data files into one dataset

The instructions below contains the codes required to download all the data files available at NHANES webpage since 1999. The rvest package is used for facilitating web scraping in R that can be also used for downloading the dataset required.

Initially, the links for examination,laboratory, demographic and dietary data are created , using pivot_longer the the created dataset is reshaped to be used for downloading the data. Thanks to the map() and unnest() functions these tasks can be done using a few lines of codes.

## 2.1. identifying the links to the datasets and downloading their links

```{r}

# initial dataset including the primary links to the files locations 
file_list <- tibble(
  BeginYear = seq.int(from = 1999,to = 2017,by = 2), ## the data is available for 1999-2000, 2001-2003, etc.
  EndYear = seq.int(from = 2000,to = 2018,by = 2),## the data is available for 1999-2000, 2001-2003, etc.
  Year = paste0(BeginYear,sep="-",EndYear), 
  examineation_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=",BeginYear),
  labratory_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear=",BeginYear),
  demographic_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=",BeginYear), 
  dietary_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Dietary&CycleBeginYear=",BeginYear) 
)

# pivot longer to reshape dataset above so we can map a function to the links
file_list <- file_list %>%
  pivot_longer(cols = contains("links"),names_to="name",values_to="page")

## function get_links_name so it can get the address for the datasets names
get_links_name <- function(page) {
 links <- read_html(page)
 links %>% 
    html_nodes("td.text-left") %>%
    html_text() %>%
    as.tibble() %>%
   mutate(file_name= value) %>%
   select(-value)
}

## function get_links_locations so it can get the address for the datasets locations

get_links_locations <- function(page) {
 links <- read_html(page)
 links %>% 
    html_nodes(".text-center+ .text-center a") %>%
    html_attr("href") %>% 
    as_tibble() %>%
    mutate(download_links=paste0("https://wwwn.cdc.gov",value)) 
}


## Scraping the links (names and locations)
file_list1 <- file_list %>% 
  mutate(files_names = map(page, get_links_name),
         files_location = map(page,get_links_locations )
         )
## unnest the links (names and locations)
file_list2 <- file_list1 %>%
  unnest(c(files_location,files_names) ,names_repair = "minimal")

## keep the files whose types are XPT and drop possible duplicates
file_list3 <- file_list2 %>%
  filter(download_links == stringr::str_match_all(string = download_links,pattern = ".*XPT")) %>%
  distinct(download_links,.keep_all = T)

```

## 2.2. Download the data files

The links to the location of the files were identified above. However, if we need to download the the datasets we are required to takes a few more steps. One can use the codes below to download all the files based on the data type (e.g. examination or demographic).

```{r}

# the links to the examination datasets for they years of 2001-2002 and 2017-2018 

examination_links <- file_list3 %>%
  filter(name == "examineation_links",BeginYear %in% c(2001,2017))

# the links to the demographics datasets for they years of 2001-2002 and 2017-2018 

demographic_links <- file_list3 %>%
  filter(name == "demographic_links",BeginYear %in% c(2001,2017)) 
  

## A function to read the data with XPT type from the links  
import_xpt <- function(links) {
  df <- haven::read_xpt(links)
  df
}


## download the demographic files

demographic_data <- demographic_links %>%
  mutate(data1 = map(download_links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")


## note: this is a very large dataset (142,875 obs and 1885 columns). So, if you want to have all the years from 1999-2000 till 2017-2018 you need more or less about 10 GB of space just for examination data

examination_data <- examination_links %>%
  mutate(data1 = map(download_links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")

```

## 2.3. Fixing the names

Replacing the variable names to their labels, can be done using the codes below. The same procedure as section 2.2. is followed here. However, later in Section 2.4, setnames() function of data.table is used for replacing the variable names with their labels.

```{r}
## create a dataset including the links to the data
name_list <-  tibble(
  BeginYear = seq.int(from = 1999,to = 2017,2),
  EndYear = seq.int(from = 2000,to = 2018,2),
  Year = paste0(BeginYear,sep="-",EndYear), 
  demo_name_link = glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear={BeginYear}"),
  examination_name_link =  glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear={BeginYear}"), 
  
  lab_name_link = glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear={BeginYear}")
  
  )

# pivot so we can map the functins to get the variable names and labels. 
 
name_list <- name_list %>%
  pivot_longer(cols = contains("link"),names_to="name",values_to="page")


## Function to get the links to the codebooks 
names_get <- function(page) {
  links <- read_html(page)
  links %>% 
    html_nodes(".text-left+ .text-center a") %>%
    html_attr("href") %>% 
    as_tibble() %>% 
    rename("links" = "value") %>%
    mutate(links=paste0("https://wwwn.cdc.gov",links)) %>%
    distinct(links,.keep_all = T)

}



#Scraping the links
name_list1 <- name_list %>% 
  mutate(name_location = map(page, names_get))  %>%
  unnest(name_location,names_repair = "minimal") 

```

## 2.4. extract the names of the variables and their labels from codebooks.

```{r}

## function to scrape codebook
codebook_get <- function(page) {
  links <- read_html(page)
  links %>% 
    html_nodes("#CodebookLinks a") %>%
    html_text() %>%
    as_tibble(.names_repair="minimal") %>% 
       rename("code" = "value")
}

## make a dataset including the codebooks 
name_list2 <- name_list1 %>% 
  mutate(code_location = map(links, codebook_get))  %>%
  unnest(code_location,names_repair = "minimal")

name_list3 <- name_list2  %>%
   separate(col = code,into = c("var_name","var_label"),sep = "-",remove = T)

## use str_squish to drop the possible white space. 
name_list3 <- name_list3 %>%
  mutate(var_name = stringr::str_squish(var_name),
         var_label=stringr::str_squish(var_label))


```

## 2.4.1. variables Name Repair: demographic file example

Here you can find the codes to change the names based on their labels for demographic files. The same can be applied to examination and laboratory datasets.

```{r}

name_list_deomgraphic <- name_list3 %>% 
  filter(name=="demo_name_link") %>%
  filter(Year == "2001-2002"|Year=="2017-2018")

## use the janitor::clean_names() to transfer the names to lowercase
demographic_data1 <- examination_data %>%
  janitor::clean_names() %>%
      as.data.frame()

## use the tolower() function to transfer the names to lowercase

name_list_deomgraphic <- name_list_deomgraphic %>%
  mutate(var_name = tolower(var_name)) %>%
      as.data.frame() 



## kee the distinct variable names 
name_list_deomgraphic <- name_list_deomgraphic %>%
  distinct(var_name,.keep_all = T)

## use setnames() of data.table to change the name to labels (for better understanding)


demographic_data1 <- setDT(demographic_data1)

setnames(demographic_data1,old = as.character(name_list_deomgraphic$var_name),new =as.character(name_list_deomgraphic$var_label),skip_absent = T )

# final data to use
demographic_data1 <- demographic_data1 %>%
  clean_names() %>%
  select(-begin_year, -end_year,-page,-links)

```

## 2.4.2. Names repair: examination file

```{r}
name_list_examination <- name_list3 %>% 
  filter(name=="examination_name_link") %>%
  filter(Year == "2001-2002"|Year=="2017-2018")

## use the janitor::clean_names() to transfer the names to lowercase
## ## also since the data set is relatively large and we only care about the variables names I keep the 1000 observations form eachyear

examination_data1 <- examination_data %>%
  janitor::clean_names() %>%
  group_by(year) %>%
  sample_n(1000) %>%
    as.data.frame() %>%
  ungroup()

## use the tolower() function to transfer the names to lowercase

name_list_examination <- name_list_examination %>%
  mutate(var_name = tolower(var_name)) %>%
      as.data.frame() 



## keep the distinct variable names 
name_list_examination <- name_list_examination %>%
  distinct(var_name,.keep_all = T)

## use setnames() of data.table to change the name to labels (for better understanding)


setnames(examination_data1,old = as.character(name_list_examination$var_name),new =as.character(name_list_examination$var_label),skip_absent = T )

# final data to use
examination_data1 <- examination_data1 %>%
  clean_names() %>%
  select(-begin_year, -end_year,-page,-links)

```
