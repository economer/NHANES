---
title: "Download NHANES"
author: "S.H.Hosseini"
date: "25/11/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# National Health and Nutrition Examination Survey (NHANES)

The National Health and Nutrition Examination Survey (NHANES) includes a series of data related the health and nutritional status of adults and children in the United States. You can find more information about NHANES here: https://www.cdc.gov/nchs/nhanes/about_nhanes.htm

The data is available online and free of use. Here I only show how to download and join a couple of datasets. However, you can follow instructions below and combine as many as files as you require. 

# The pacakges required
To download and clean the data we require haven and tidyverse . Just in case we need to work with date and time vars we can install and load lubridate and hms. I also prefer to clean the vars names so I load janiotr to be able to use clean_names() function. 

```{r}
library(haven)
library(tidyverse)
library(lubridate)
library(hms)
library(janitor)
library(glue)
library(rvest)
library(stringr)
library(data.table)
```


# Download the files required

NHANES includes several datasets including demographic data, dietary data, examination data and laboratory data. Each dataset could also include several files. For instance demographic dataset includes only one file. However, the examination data contains information about blood pressure, body measures, oral health etc. 

Here, we focus on few files including demographic, blood pressure, body measures, and finally insulin an iron status (from laboratory datasets). These datasets were all collected in 2017-2018. Although, NHANES is not a longitudinal dataset, many studies tend to pool NAHNES data over years and I will show you how to do it later. 

## Download the demographic data
To download the demographic data we can simply use the online address of the file. You could go to:

https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=2017

, and then find the location of file under "Data File" that is in "xpt" format. 


```{r}
demo_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT") %>%
  clean_names()

```


demo_2017 includes several vars and I will not change the vars names for now because we are going to append 2017-2018 with other datasets so, we would better keep the columns as they are. 

## Download blood pressure and body measures from Examination Data along with insulin and iron status from Laboratory Dataset. 

The Examination Data for 2017-2018 can be found here: https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=2017

[The Examination Data for 2017-2018 can be found here:] (https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=2017)


The Laboratory Data for 2017-2018 can be found here: 
https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear=2017

It should be noted that there will be several missing values in the final dataset. This is because not every person participated in the primary sample was eligible to participate in the all the examinations or laboratory tests conducted. For instance, in the case of insulin data, only individuals above 12 years of age were taken into consideration.


```{r}
# blood pressure data
blood_pressure_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/BPX_J.XPT") %>%
  clean_names()

## body measures data
body_measures_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/BMX_J.XPT") %>%
  clean_names()

## insulin data
insulin_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/INS_J.XPT") %>%
  clean_names()

## iron status 
iron_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/FETIB_J.XPT") %>%
  clean_names()
```

## Joining the datasets 

Each individual is assigned an ID stored in the seqn var and we are going to use seqn for joining the downloaded datasets. Here we can use left_join() function in dplyr however, it takes some times to include every dataset (example is provided below). So, it seems that join_all() function from plyr package is a less time consuming choice. Note that you are supposed to choose the type of JOIN in plyr::join_all() function that is left here. 

```{r}
## using dplyr to join 3 datasts. 
nh_2017_dplyr <- left_join(x = demo_2017,y = blood_pressure_2017,by="seqn") %>%
              left_join(.,body_measures_2017,by="seqn")
    
## using join_all from plyr. 

nh_2017 <- plyr::join_all(dfs = list(demo_2017,body_measures_2017,blood_pressure_2017,iron_2017,insulin_2017),by = "seqn",type = "left")

```



## Cleaning the created dataset

The file created here includes 9254 observations and 98 variables. Using the documentations for each file I am going to keep only a subset of columns as an example. 

For instance the 2017-2018 documentation of demographic file can be found here: 

## Variable in Deomgraphic File

```{r}
nh_2017 <- nh_2017 %>%
  select(seqn,
         cycle = sddsrvyr,
         gender = riagendr,
         age = ridageyr, 
         race = ridreth3, 
         birth_country = 
         )


## race=1 -> mexaican-american,race=2 -> hispanic, race=3 -> white, race=4 -> black, race=5 -> 
```

# Importing all data files into one dataset

Below, you could find the codes to download all the data files available at NHANES webpage since 1999. I used rvest package facilitating web scraping in R.  

Initially, I created the links for examination,laboratory, demographic and dietary data, then I reshape the dataset incluidng the links so I can map a function to each link.  

## links to the datasets
```{r}
file_list <- tibble(
  BeginYear = seq.int(from = 1999,to = 2017,2),
  EndYear = seq.int(from = 2000,to = 2018,2),
  Year = paste0(BeginYear,sep="-",EndYear), 
  examineation_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=",BeginYear),
  labratory_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear=",BeginYear),
  demographic_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=",BeginYear), 
  dietary_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Dietary&CycleBeginYear=",BeginYear) 
)

# pivot so we can map the functions to the links 
file_list <- file_list %>%
  pivot_longer(cols = contains("links"),names_to="name",values_to="page")

## function links_get so it can get the address for the datasets
links_get <- function(page) {
  links <- read_html(page)
  links %>% 
    html_nodes(".text-center+ .text-center a") %>%
    html_attr("href") %>% 
    as_tibble() %>% 
    rename("links" = "value") %>%
    mutate(links=paste0("https://wwwn.cdc.gov",links)) %>%
    distinct(links,.keep_all = T)

}



#Scrape the links
file_list1 <- file_list %>% 
  mutate(files_location = map(page, links_get))

## unnest the links 
file_list1 <- file_list1 %>%
  unnest(files_location,names_repair = "minimal") 

## keep the files whose types are XPT
file_list1 <- file_list1 %>%
  filter(links == stringr::str_match_all(string = links,pattern = ".*XPT")) %>%
  distinct(links,.keep_all = T)



# the links to the examination datasets for they years of 2001-2002 and 2017-2018 

examination_links <- file_list1 %>%
  filter(name == "examineation_links",BeginYear %in% c(2001,2017))

# the links to the demographics datasets for they years of 2001-2002 and 2017-2018 

demographic_links <- file_list1 %>%
  filter(name == "demographic_links",BeginYear %in% c(2001,2017)) 
  

## A function to read the data with XPT type from the links  
import_xpt <- function(links) {
  df <- haven::read_xpt(links)
  df
}


## download the demographic files

demographic_data <- demographic_links %>%
  mutate(data1 = map(links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")


## note: this is a very large dataset (142,875 obs and 1885 columns). So, if you want to have all the years from 1999-2000 till 2017-2018 you need more or less about 10 GB of space just for examination data

examination_data <- examination_links %>%
  mutate(data1 = map(links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")

```

## Fixing the names

If you would like to replace the variable names to their lable, you can run the following the codes. 

```{r}
## create a dataset including the links to the data
name_list <-  tibble(
  BeginYear = seq.int(from = 1999,to = 2017,2),
  EndYear = seq.int(from = 2000,to = 2018,2),
  Year = paste0(BeginYear,sep="-",EndYear), 
  demo_name_link = glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear={BeginYear}"),
  examination_name_link =  glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear={BeginYear}"), 
  
  lab_name_link = glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear={BeginYear}")
  
  )

# pivot so we can map the functins to get the varaible names and labels. 
 
name_list <- name_list %>%
  pivot_longer(cols = contains("link"),names_to="name",values_to="page")


## Function to get the links to the codebooks 
names_get <- function(page) {
  links <- read_html(page)
  links %>% 
    html_nodes(".text-left+ .text-center a") %>%
    html_attr("href") %>% 
    as_tibble() %>% 
    rename("links" = "value") %>%
    mutate(links=paste0("https://wwwn.cdc.gov",links)) %>%
    distinct(links,.keep_all = T)

}



#Scrape the links
name_list1 <- name_list %>% 
  mutate(name_location = map(page, names_get))  %>%
  unnest(name_location,names_repair = "minimal") 

```

## extract the names of the varaibles and their labels from codebooks.

```{r}

## function to scrape codebook
codebook_get <- function(page) {
  links <- read_html(page)
  links %>% 
    html_nodes("#CodebookLinks a") %>%
    html_text() %>%
    as_tibble(.names_repair="minimal") %>% 
       rename("code" = "value")
}

## make a dataset including the codebooks 
name_list2 <- name_list1 %>% 
  mutate(code_location = map(links, codebook_get))  %>%
  unnest(code_location,names_repair = "minimal")

name_list3 <- name_list2  %>%
   separate(col = code,into = c("var_name","var_label"),sep = "-",remove = T)

## use str_squish to drop the possible white space. 
name_list3 <- name_list3 %>%
  mutate(var_name = stringr::str_squish(var_name),
         var_label=stringr::str_squish(var_label))

name_list3 %>%
  mutate(Year = str_squish(Year)) %>%
  count(Year)
```

## Varaibles Name Repair: demographic file example 

Here you can find the codes to change the names based on their labels for demographic files. The same can be applied to examination and laboratory data and all years.  

```{r}


name_list_deomgraphic <- name_list3 %>% 
  filter(name=="demo_name_link") %>%
  filter(Year == "2001-2002"|Year=="2017-2018")

## use the janitor::clean_names() to transfer the names to lowercase
demographic_data1 <- examination_data %>%
  janitor::clean_names() %>%
      as.data.frame()

## use the tolower() function to transfer the names to lowercase

name_list_deomgraphic <- name_list_deomgraphic %>%
  mutate(var_name = tolower(var_name)) %>%
      as.data.frame() 



## kee the distinct var names 
name_list_deomgraphic <- name_list_deomgraphic %>%
  distinct(var_name,.keep_all = T)

## use setnames() of data.table to change the name to labels (for better understanding)


demographic_data1 <- setDT(demographic_data1)
setnames(demographic_data1,old = as.character(name_list_deomgraphic$var_name),new =as.character(name_list_deomgraphic$var_label),skip_absent = T )

# final data to use
demographic_data1 <- demographic_data1 %>%
  clean_names() %>%
  select(-begin_year, -end_year,-page,-links)

```



## Names repair examination file
```{r}
name_list_examination <- name_list3 %>% 
  filter(name=="examination_name_link") %>%
  filter(Year == "2001-2002"|Year=="2017-2018")

## use the janitor::clean_names() to transfer the names to lowercase
## ## also since the data set is relatively large and we only care about the variables names I keep the 1000 observations form eachyear

examination_data1 <- examination_data %>%
  janitor::clean_names() %>%
  group_by(year) %>%
  sample_n(1000) %>%
    as.data.frame() %>%
  ungroup()

## use the tolower() function to transfer the names to lowercase

name_list_examination <- name_list_examination %>%
  mutate(var_name = tolower(var_name)) %>%
      as.data.frame() 



## keep the distinct var names 
name_list_examination <- name_list_examination %>%
  distinct(var_name,.keep_all = T)

## use setnames() of data.table to change the name to labels (for better understanding)


setnames(examination_data1,old = as.character(name_list_examination$var_name),new =as.character(name_list_examination$var_label),skip_absent = T )

# final data to use
examination_data1 <- examination_data1 %>%
  clean_names() %>%
  select(-begin_year, -end_year,-page,-links)
examination_data1 %>%
  ungroup() %>%
  View()
```

