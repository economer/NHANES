---
title: "Downloading National Health and Nutrition Examination Survey (NHANES) Files"
author: "S.H.Hosseini"
date: "25/11/2020"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. National Health and Nutrition Examination Survey (NHANES): Downloading and Joining Individual files.

The National Health and Nutrition Examination Survey (NHANES) includes a series of data related the health and nutritional status of adults and children in the United States. You can find more information about NHANES here: <https://www.cdc.gov/nchs/nhanes/about_nhanes.htm>

The data is available online and free of use. Section 1 includes codes required to download and join individual datasets from the NHANES webpage. Section 2 can be used for downloading and joining all the data available since 1999.

## 1.2. The pacakges required

To download and clean the individual data files we require "haven" and "tidyverse". One can also use "janitor" package to use clean_names() function.

For the second task the primary packages required are glue, rvest, stringr and data.table.

```{r,echo=F}
library(haven)
library(tidyverse)
library(janitor)
library(glue)
library(rvest)
library(stringr)
library(data.table)
```

## 1.3. Download the files required

NHANES includes several datasets including demographic data, dietary data, examination data and laboratory data. Each dataset could also include several files. For instance demographic dataset includes only one file. However, the examination data contains information about blood pressure, body measures, oral health etc.

Here, we focus on few files including demographic, blood pressure, body measures, insulin and finally iron status (from laboratory datasets). These datasets were all collected in 2017-2018. Although, NHANES is not a longitudinal dataset, many studies tend to pool NAHNES data over years.

### 1.3.1. Download the demographic data

To download the demographic data one can simply use the online address of the file at:

[Files locations](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=2017)

,and then find the location of file under "Data File" that is in "xpt" format.

```{r}

demo_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT") %>%
  clean_names()

```

demo_2017 includes several variables that might be confusing to use without knowing the labels. However, it is possible to use setnames() function of data.table to change the variables' names to their labels. In Section 2, it will be shown how to perfom this taks.

### 1.3.2. Download blood pressure and body measures from Examination Data along with insulin and iron status from Laboratory Dataset.

The Examination Data for 2017-2018 can be found here: <https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=2017>

The Laboratory Data for 2017-2018 can be found here: <https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear=2017>

It should be noted that there will be several missing values in the final dataset. This is because not every person participated in the primary sample (i.e. demographic) was eligible to participate in all of the examinations or laboratory tests conducted. For instance, in the case of insulin data, only individuals above 12 years of age were eligible to be tested for measuring insulin levels.

```{r}

# blood pressure data
blood_pressure_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/BPX_J.XPT") %>%
  clean_names()

## body measures data
body_measures_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/BMX_J.XPT") %>%
  clean_names()

## insulin data
insulin_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/INS_J.XPT") %>%
  clean_names()

## iron status 
iron_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/FETIB_J.XPT") %>%
  clean_names()
```

## 1.4. Joining the datasets

Each individual is assigned an ID stored in the seqn variable that can be used for joining the downloaded datasets. Here one can use left_join() function in dplyr however, it takes some times to include every dataset (example is provided below). So, it seems that join_all() function from plyr package is a less time consuming choice. Note that the type of JOIN in plyr::join_all() function should be chosen that is "left" in this case.

**Note**: As it is noted by plyr package documentation "loading plyr and dplyr packages in the same time is likely to cause problems.If you need functions from both plyr and dplyr, please load plyr first, then dplyr"

```{r}
library(plyr)
library(dplyr)
## using dplyr to join 3 datasts. 
nh_2017_dplyr <- left_join(x = demo_2017,y = blood_pressure_2017,by="seqn") %>%
              left_join(.,body_measures_2017,by="seqn")
    
## using join_all from plyr. 

nh_2017 <- plyr::join_all(dfs = list(demo_2017,body_measures_2017,blood_pressure_2017,iron_2017,insulin_2017),by = "seqn",type = "left")

```

# 2. Importing all data files

The instructions below contains the codes required to download all the data files available at NHANES webpage since 1999. The rvest package is used for facilitating web scraping in R that can be also used for downloading the dataset required.

Initially, the links for examination,laboratory, demographic and dietary data are created , using pivot_longer the the created dataset is reshaped to be used for downloading the data. Thanks to the map() and unnest() functions these tasks can be done using a few lines of codes.

## 2.1. identifying the links to the datasets and downloading their links

```{r}

# initial dataset including the primary links to the files locations 
file_list <- tibble(
  BeginYear = seq.int(from = 1999,to = 2017,by = 2), ## the data is available for 1999-2000, 2001-2003, etc.
  EndYear = seq.int(from = 2000,to = 2018,by = 2),## the data is available for 1999-2000, 2001-2003, etc.
  Year = paste0(BeginYear,sep="-",EndYear), 
  examineation_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=",BeginYear),
  labratory_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear=",BeginYear),
  demographic_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=",BeginYear), 
  dietary_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Dietary&CycleBeginYear=",BeginYear) 
)

# pivot longer to reshape dataset above so we can map a function to the links

file_list <- file_list %>%
  pivot_longer(cols = contains("links"),names_to="name",values_to="page")

## function get_links_name for getting the address for the datasets names. (see rvest package documentation for more information)

get_links_name <- function(page) {
 links <- read_html(page)
 links %>% 
    html_nodes("td.text-left") %>%
    html_text() %>%
    as_tibble() %>%
   mutate(file_name= value) %>%
   select(-value)
}

## function get_links_locations for getting the address of the datasets locations.

get_links_locations <- function(page) {
 links <- read_html(page)
 links %>% 
    html_nodes(".text-center+ .text-center a") %>%
    html_attr("href") %>% 
    as_tibble() %>%
    mutate(download_links=paste0("https://wwwn.cdc.gov",value)) 
}


## Scraping the links (names and locations)
file_list1 <- file_list %>% 
  mutate(files_names = map(page, get_links_name),
         files_location = map(page,get_links_locations )
         )
## unnest the links (names and locations)
file_list2 <- file_list1 %>%
  unnest(c(files_location,files_names) ,names_repair = "minimal")

## keep the files whose types are XPT and drop possible duplicates
file_list3 <- file_list2 %>%
  filter(download_links == stringr::str_match_all(string = download_links,pattern = ".*XPT")) %>%
  distinct(download_links,.keep_all = T)

#write_csv(file_list3,"file_list3.csv")
#file_list3 <- read_csv("file_list3.csv")
#
```

## 2.2. Download the data files

The links to the location of the files were identified above. However, if we need to download the the datasets we are required to take a few more steps. One can use the codes below to download all the files based on the data type (e.g. examination or demographic) and file name.

Usually a few subsets of the examination or laboratory files are required for a study. Therefore creating a dataset for each file seems to be tidyer than downloading all files at once.

```{r}
file_list3 %>%
  ungroup() %>%
  count(name, file_name)
```

```{r}

# the links to the examination datasets for blood pressure files 
# here the str_detect is used for filtering file names containing Blood Pressure.

examination_links <- file_list3 %>%
  filter(name == "examineation_links",
         str_detect(string = file_name,pattern = "^[bB]lood [pP]ressure.*")
         )

# the links to the laboratory datasets for Albumin & Creatinine - Urine files 
# here the str_detect is used for filtering file names containing Albumin
laboratory_links <- file_list3 %>%
  filter(name == "labratory_links",
         str_detect(string = file_name,pattern = "^Albumin")
         )


# the links to the demographics datasets

demographic_links <- file_list3 %>%
  filter(name == "demographic_links") 
  

## A function to read the data with XPT type from the links  
import_xpt <- function(links) {
  df <- haven::read_xpt(links)
  df
}


## download the demographic files

demographic_data <- demographic_links %>%
  mutate(data1 = map(download_links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")


## download the examination files

examination_data <- examination_links %>%
  mutate(data1 = map(download_links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")


## download the laboratory files

laboratory_data <- examination_links %>%
  mutate(data1 = map(download_links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")


## Joining the datasets. 
joined_dfs <- plyr::join_all(dfs = list(demographic_data,examination_data,laboratory_data),by = "SEQN",type = "full")

joined_dfs %>%
  View()
```

## 2.3. Fixing the names

Replacing the variable names to their labels, can be done using the codes below. The same procedure as section 2.2. is followed here. However, later in Section 2.4, setnames() function of data.table is used for replacing the variable names with their labels.

```{r}
## create a dataset including the links to the datasets locations
name_list <-  tibble(
  BeginYear = seq.int(from = 1999,to = 2017,2),
  EndYear = seq.int(from = 2000,to = 2018,2),
  Year = paste0(BeginYear,sep="-",EndYear), 
  demo_name_link = glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear={BeginYear}"),
  examination_name_link =  glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear={BeginYear}"), 
  
  lab_name_link = glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear={BeginYear}")
  
  )

# pivot so we can map the functins to get the variable names and labels. 
 
name_list <- name_list %>%
  pivot_longer(cols = contains("link"),names_to="name",values_to="page")


## Function to get the links to the codebooks 
get_names <- function(page) {
  links <- read_html(page)
  links %>% 
    html_nodes(".text-left+ .text-center a") %>%
    html_attr("href") %>% 
    as_tibble() %>% 
    rename("links" = "value") %>%
    mutate(links=glue::glue("https://wwwn.cdc.gov{links}")) 

}


#Scraping the code books links 

name_list1 <- name_list %>% 
  mutate(name_location = map(page, get_names)
         )  %>%
  unnest(name_location,names_repair = "minimal") %>%
  filter(str_detect(string = links,pattern = ".*htm.*"))

```

## 2.4. Extract the names of the variables and their labels from codebooks.

```{r}

## function for scraping the codebooks
codebook_get <- function(page) {
  links <- read_html(page)
  Sys.sleep(time = 0.01)
  links %>% 
    html_nodes("#CodebookLinks a") %>%
    html_text() %>%
    as_tibble(.names_repair="minimal") %>% 
       rename("code" = "value")
}

## make a dataset including the codebooks 
name_list2 <- name_list1 %>% 
  mutate(code_location = map(links, codebook_get))  %>%
  unnest(code_location,names_repair = "minimal")



## use separate() function to separate code to var name and var labels
name_list3 <- separate(data = name_list2, col = code,into = c("var_name","var_label"), sep = " - ",extra = "merge") 



## use str_squish to drop the possible white space. 
name_list3 <- name_list3 %>%
  mutate(var_name = stringr::str_squish(var_name),
         var_label=stringr::str_squish(var_label))

## write and read the created file


#write_csv(x = name_list2,"name_list2.csv")
#name_list2 <- read_csv("name_list2.csv")

#write_csv(x = name_list3,"name_list3.csv")
#name_list3 <- read_csv("name_list3.csv")
#


```

## 2.5. variables Name Repair:

Here you can find the codes to change the names based on their labels for all variables in joined_df dataset.

```{r}

## transform the variable names and variables labels to lower case and drop the duplicates
name_list3 <- name_list3 %>%
  mutate(
    var_name = tolower(var_name),
    var_label = tolower(var_label)) %>%
  distinct(var_name,.keep_all = T) 

## transform the variable names to lower case
joined_dfs <- joined_dfs %>%
  clean_names()

## use setnames() to change the varibale names to their labels. 
##  
setnames(joined_dfs,old = as.character(name_list3$var_name),new =as.character(name_list3$var_label),skip_absent = T )

# final data to use
joined_dfs1 <- joined_dfs %>%
    as_tibble(.name_repair = "minimal") 

name_list3 %>%
  ungroup() %>%
  as_tibble() %>%
  group_by(var_label) %>%
  dplyr::count(sort = T)
```


## 2.6. Creating a dataset including body measures from the Examination Datasets along with insulin from Laboratory Datasets

Two sources created in sections 2.1 and 2.3 that are file_list3 and name_list3 are required for this task. First the required files' locations are selected and downloaded. Consequently, the names are fixed based on the name_list dataset.

we use str_detect to create 3 variables called filter1, filter2 and filter3 that are TRUE if the names contain Insulin, Body Measure and Demographic. Since, the regex here can include every expression with insulin and body measure, we will see that two files including Glycohemoglobin, Plasma Glucose, Serum C-peptid and Arthritis Body Measures are also included in the selected_data. Therefore, they are filtered out.

```{r}

# use str_detect to filter the files required
selected_data <-  file_list3 %>%
  mutate(filter1 = str_detect(string = file_name,pattern = ".*[I]nsulin*"), 
         filter2 = str_detect(string = file_name,pattern = "[bB]ody Measure.*" ),
        filter3 = str_detect(string = file_name,pattern = ".*[dD]emographic.*"),
         ) %>%
  filter(filter1 ==T| filter2==T| filter3==T) %>%
  filter(!str_detect(string = file_name, pattern = ".*Glycohemoglobin, Plasma Glucose, Serum C-peptid.*")) %>%
  filter(!str_detect(string = file_name,pattern = ".*Arthritis Body Measures.*")) %>%
  filter(BeginYear %in% c(1999,2017))



## function to download xpt files 
 import_xpt <- function(links) {
  df <- haven::read_xpt(links)
  
 }
 
## use map to map the import_xpt() to the selected_data 
selected_data_map <- selected_data %>%
  mutate(data_files = map(download_links,import_xpt))

selected_data_map_unnest <- selected_data_map %>%
  unnest(data_files)

selected_data_map_unnest %>%
  clean_names() %>%
  View()

selected_data_map <-selected_data %>%
  mutate(data_files = map(download_links,import_xpt)) #%>%
  unnest(data_files)
  


```



### 2.6.1. Name repair

```{r}

name_list3 <- name_list3 %>%
  distinct(var_name,.keep_all = T)


setnames(x = selected_data_map,old = as.character(name_list3$var_name),new = as.character(name_list3$var_label),skip_absent = T)

selected_data_map <- selected_data_map %>%
  clean_names() %>%
  select(-page, -value, -download_links,-filter1, -filter2, -filter3)

## final dataset: 
selected_data_map <- selected_data_map %>%
  as_tibble(.name_repair = "minimal")
  

```

