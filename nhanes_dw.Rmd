---
title: "Downloading National Health and Nutrition Examination Survey (NHANES) Files"
author: "S.H.Hosseini"
date: "25/11/2020"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. National Health and Nutrition Examination Survey (NHANES): Downloading and Joining Individual files.

The National Health and Nutrition Examination Survey (NHANES) includes a series of data related the health and nutritional status of adults and children in the United States. You can find more information about NHANES here: <https://www.cdc.gov/nchs/nhanes/about_nhanes.htm>

The data is available online and free of use. Section 1 includes codes required to download and join individual datasets from the NHANES webpage. Section 2 can be used for downloading and joining all the data available since 1999.

## 1.2. The pacakges required

To download and clean the individual data files we require "haven" and "tidyverse". One can also use "janitor" package to use clean_names() function.

For the second task the primary packages required are glue, rvest, stringr and data.table.

```{r,echo=F}
library(haven)
library(tidyverse)
library(janitor)
library(glue)
library(rvest)
library(stringr)
library(data.table)
```

## 1.3. Download the files required

NHANES includes several datasets including demographic data, dietary data, examination data and laboratory data. Each dataset could also include several files. For instance demographic dataset includes only one file. However, the examination data contains information about blood pressure, body measures, oral health etc.

Here, we focus on few files including demographic, blood pressure, body measures, insulin and finally iron status (from laboratory datasets). These datasets were all collected in 2017-2018. Although, NHANES is not a longitudinal dataset, many studies tend to pool NAHNES data over years.

### 1.3.1. Download the demographic data

To download the demographic data one can simply use the online address of the file at:

[Files locations](https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=2017)

,and then find the location of file under "Data File" that is in "xpt" format.

```{r}
demo_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/DEMO_J.XPT") %>%
  clean_names()

```

demo_2017 includes several variables that might be confusing to use without knowing the labels. However, it is possible to use setnames() function of data.table to change the variables' names to their labels. In Section 2, it will be shown how to perfom this taks.

### 1.3.2. Download blood pressure and body measures from Examination Data along with insulin and iron status from Laboratory Dataset.

The Examination Data for 2017-2018 can be found here: <https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=2017>

The Laboratory Data for 2017-2018 can be found here: <https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear=2017>

It should be noted that there will be several missing values in the final dataset. This is because not every person participated in the primary sample (i.e. demographic) was eligible to participate in all of the examinations or laboratory tests conducted. For instance, in the case of insulin data, only individuals above 12 years of age were eligible to be tested for measuring insulin levels.

```{r}
# blood pressure data
blood_pressure_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/BPX_J.XPT") %>%
  clean_names()

## body measures data
body_measures_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/BMX_J.XPT") %>%
  clean_names()

## insulin data
insulin_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/INS_J.XPT") %>%
  clean_names()

## iron status 
iron_2017 <- read_xpt(file = "https://wwwn.cdc.gov/Nchs/Nhanes/2017-2018/FETIB_J.XPT") %>%
  clean_names()
```

## 1.4. Joining the datasets

Each individual is assigned an ID stored in the seqn variable that can be used for joining the downloaded datasets. Here one can use left_join() function in dplyr however, it takes some times to include every dataset (example is provided below). So, it seems that join_all() function from plyr package is a less time consuming choice. Note that the type of JOIN in plyr::join_all() function should be chosen that is "left" in this case.

**Note**: As it is noted by plyr package documentation "loading plyr and dplyr packages in the same time is likely to cause problems.If you need functions from both plyr and dplyr, please load plyr first, then dplyr"

```{r}
library(plyr)
library(dplyr)
## using dplyr to join 3 datasts. 
nh_2017_dplyr <- left_join(x = demo_2017,y = blood_pressure_2017,by="seqn") %>%
              left_join(.,body_measures_2017,by="seqn")
    
## using join_all from plyr. 

nh_2017 <- plyr::join_all(dfs = list(demo_2017,body_measures_2017,blood_pressure_2017,iron_2017,insulin_2017),by = "seqn",type = "left")

```

# 2. Importing all data files

The instructions below contains the codes required to download all the data files available at NHANES webpage since 1999. The rvest package is used for facilitating web scraping in R that can be also used for downloading the dataset required.

Initially, the links for examination,laboratory, demographic and dietary data are created , using pivot_longer the the created dataset is reshaped to be used for downloading the data. Thanks to the map() and unnest() functions these tasks can be done using a few lines of codes.

## 2.1. identifying the links to the datasets and downloading their links

```{r}

# initial dataset including the primary links to the files locations 
file_list <- tibble(
  BeginYear = seq.int(from = 1999,to = 2017,by = 2), ## the data is available for 1999-2000, 2001-2003, etc.
  EndYear = seq.int(from = 2000,to = 2018,by = 2),## the data is available for 1999-2000, 2001-2003, etc.
  Year = paste0(BeginYear,sep="-",EndYear), 
  examineation_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear=",BeginYear),
  labratory_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear=",BeginYear),
  demographic_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear=",BeginYear), 
  dietary_links = paste0("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Dietary&CycleBeginYear=",BeginYear) 
)

# pivot longer to reshape dataset above so we can map a function to the links

file_list <- file_list %>%
  pivot_longer(cols = contains("links"),names_to="name",values_to="page")

## function get_links_name for getting the address for the datasets names. (see rvest package documentation for more information)

get_links_name <- function(page) {
 links <- read_html(page)
 links %>% 
    html_nodes("td.text-left") %>%
    html_text() %>%
    as.tibble() %>%
   mutate(file_name= value) %>%
   select(-value)
}

## function get_links_locations for getting the address of the datasets locations.

get_links_locations <- function(page) {
 links <- read_html(page)
 links %>% 
    html_nodes(".text-center+ .text-center a") %>%
    html_attr("href") %>% 
    as_tibble() %>%
    mutate(download_links=paste0("https://wwwn.cdc.gov",value)) 
}


## Scraping the links (names and locations)
file_list1 <- file_list %>% 
  mutate(files_names = map(page, get_links_name),
         files_location = map(page,get_links_locations )
         )
## unnest the links (names and locations)
file_list2 <- file_list1 %>%
  unnest(c(files_location,files_names) ,names_repair = "minimal")

## keep the files whose types are XPT and drop possible duplicates
file_list3 <- file_list2 %>%
  filter(download_links == stringr::str_match_all(string = download_links,pattern = ".*XPT")) %>%
  distinct(download_links,.keep_all = T)

```

## 2.2. Download the data files

The links to the location of the files were identified above. However, if we need to download the the datasets we are required to take a few more steps. One can use the codes below to download all the files based on the data type (e.g. examination or demographic).

**NOTE**: If you are interested in a downloading individual datasets such as body measures, insulin, demographic and so on across specific years, you can jump to Section 2.5.

```{r}

# the links to the examination datasets for they years of 2001-2002 and 2017-2018 

examination_links <- file_list3 %>%
  filter(name == "examineation_links",BeginYear %in% c(2001,2017))

# the links to the demographics datasets for they years of 2001-2002 and 2017-2018 

demographic_links <- file_list3 %>%
  filter(name == "demographic_links",BeginYear %in% c(2001,2017)) 
  

## A function to read the data with XPT type from the links  
import_xpt <- function(links) {
  df <- haven::read_xpt(links)
  df
}


## download the demographic files

demographic_data <- demographic_links %>%
  mutate(data1 = map(download_links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")


## note: the codes below redner a very large dataset (142,875 observations and 1885 columns). So, if you want to have all the years from 1999-2000 till 2017-2018 you need more or less about 10 GB of space just for the examination data

examination_data <- examination_links %>%
  mutate(data1 = map(download_links,import_xpt))  %>%
  unnest(data1,names_repair = "minimal")

```

## 2.3. Fixing the names

Replacing the variable names to their labels, can be done using the codes below. The same procedure as section 2.2. is followed here. However, later in Section 2.4, setnames() function of data.table is used for replacing the variable names with their labels.

```{r}
## create a dataset including the links to the datasets locations
name_list <-  tibble(
  BeginYear = seq.int(from = 1999,to = 2017,2),
  EndYear = seq.int(from = 2000,to = 2018,2),
  Year = paste0(BeginYear,sep="-",EndYear), 
  demo_name_link = glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Demographics&CycleBeginYear={BeginYear}"),
  examination_name_link =  glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Examination&CycleBeginYear={BeginYear}"), 
  
  lab_name_link = glue("https://wwwn.cdc.gov/nchs/nhanes/search/datapage.aspx?Component=Laboratory&CycleBeginYear={BeginYear}")
  
  )

# pivot so we can map the functins to get the variable names and labels. 
 
name_list <- name_list %>%
  pivot_longer(cols = contains("link"),names_to="name",values_to="page")


## Function to get the links to the codebooks 
get_names <- function(page) {
  links <- read_html(page)
  links %>% 
    html_nodes(".text-left+ .text-center a") %>%
    html_attr("href") %>% 
    as_tibble() %>% 
    rename("links" = "value") %>%
    mutate(links=glue::glue("https://wwwn.cdc.gov/{links}")) 

}


#Scraping the code books links 

name_list1 <- name_list %>% 
  mutate(name_location = map(page, get_names)
         )  %>%
  unnest(name_location,names_repair = "minimal") %>%
  filter(str_detect(string = links,pattern = ".*htm.*"))
```

## 2.4. Extract the names of the variables and their labels from codebooks.

```{r}

## function for scraping the codebooks
codebook_get <- function(page) {
  links <- read_html(page)
  Sys.sleep(time = 0.01)
  links %>% 
    html_nodes("#CodebookLinks a") %>%
    html_text() %>%
    as_tibble(.names_repair="minimal") %>% 
       rename("code" = "value")
}

## make a dataset including the codebooks 
name_list2 <- name_list1 %>% 
  mutate(code_location = map(links, codebook_get))  %>%
  unnest(code_location,names_repair = "minimal")


## use separate() function to separte code to var name and var labales
name_list3 <- name_list2  %>%
   separate(col = code,into = c("var_name","var_label"),sep = "-",remove = T)

## use str_squish to drop the possible white space. 
name_list3 <- name_list3 %>%
  mutate(var_name = stringr::str_squish(var_name),
         var_label=stringr::str_squish(var_label))

## write and read the created file

#write_csv(x = name_list3,"name_list3.csv")
#name_list3 <- read_csv("name_list3.csv")
```

### 2.4.1. variables Name Repair: demographic file example

Here you can find the codes to change the names based on their labels for demographic files. The same can be applied to the examination and the laboratory datasets.

```{r}
## keep the demographic files for years 2001-2002 and 2017-2018
name_list_deomgraphic <- name_list3 %>% 
  filter(name=="demo_name_link") %>%
  filter(Year == "2001-2002"|Year=="2017-2018")

## use the janitor::clean_names() to transfer the names to lowercase
demographic_data1 <- examination_data %>%
  janitor::clean_names() %>%
      as.data.frame()

## use the tolower() function to transfer the names to lowercase

name_list_deomgraphic <- name_list_deomgraphic %>%
  mutate(var_name = tolower(var_name)) %>%
      as.data.frame() 



## kee the distinct variable names 
name_list_deomgraphic <- name_list_deomgraphic %>%
  distinct(var_name,.keep_all = T)

## use setnames() of data.table to change the name to labels (for better understanding)

# the old argument here is the var_name of the name_list_demographic dataset and the new argument is the var_lable. In this process, the var_name change to var_lable

setnames(demographic_data1,old = as.character(name_list_deomgraphic$var_name),new =as.character(name_list_deomgraphic$var_label),skip_absent = T )

# final data to use
demographic_data1 <- demographic_data1 %>%
  clean_names() %>%
  select(-begin_year, -end_year,-page,-links)

```

### 2.4.2. Names repair: the examination file

```{r}

name_list_examination <- name_list3 %>% 
  filter(name=="examination_name_link") %>%
  filter(Year == "2001-2002"|Year=="2017-2018")

## use the janitor::clean_names() to transfer the names to lowercase
## ## also since the data set is relatively large and we only care about the variables names I keep the 1000 observations form eachyear

examination_data1 <- examination_data %>%
  janitor::clean_names() %>%
  group_by(year) %>%
  sample_n(1000) %>%
    as.data.frame() %>%
  ungroup()

## use the tolower() function to transfer the names to lowercase

name_list_examination <- name_list_examination %>%
  mutate(var_name = tolower(var_name)) %>%
      as.data.frame() 



## keep the distinct variable names 
name_list_examination <- name_list_examination %>%
  distinct(var_name,.keep_all = T)

## use setnames() of data.table to change the name to labels (for better understanding)


setnames(examination_data1,old = as.character(name_list_examination$var_name),new =as.character(name_list_examination$var_label),skip_absent = T )

# final data to use
examination_data1 <- examination_data1 %>%
  clean_names() %>%
  select(-begin_year, -end_year,-page,-links)


```

## 2.5. Creating a dataset including body measures from the Examination Datasets along with insulin from Laboratory Datasets

Two sources created in sections 2.1 and 2.3 that are file_list3 and name_list3 are required for this task. First the required files' locations are selected and downloaded. Consequently, the names are fixed based on the name_list dataset.

we use str_detect to create 3 variables called filter1, filter2 and filter3 that are TRUE if the names contain Insulin, Body Measure and Demographic. Since, the regex here can include every experssion with insulin and body measure, we will see that two files including Glycohemoglobin, Plasma Glucose, Serum C-peptid and Arthritis Body Measures are also included in the selected_data. Therefore, they are filterd out.

```{r}
# use str_detect to filter the files required
selected_data <-  file_list3 %>%
  mutate(filter1 = str_detect(string = file_name,pattern = ".*[I]nsulin*"), 
         filter2 = str_detect(string = file_name,pattern = "[bB]ody Measure.*" ),
        filter3 = str_detect(string = file_name,pattern = ".*[dD]emographic.*"),
         ) %>%
  filter(filter1 ==T| filter2==T| filter3==T) %>%
  filter(!str_detect(string = file_name, pattern = ".*Glycohemoglobin, Plasma Glucose, Serum C-peptid.*")) %>%
  filter(!str_detect(string = file_name,pattern = ".*Arthritis Body Measures.*"))



## function to download xpt files 
 import_xpt <- function(links) {
  df <- haven::read_xpt(links)
  df
 }
 
## use map to map the import_xpt() to the selected_data 
selected_data_map <-selected_data %>%
  mutate(data_files = map(download_links,import_xpt)) %>%
  unnest(data_files)
```

### 2.5.1. Name repair

```{r}
name_list3 <- name_list3 %>%
  distinct(var_name,.keep_all = T)


setnames(x = selected_data,old = as.character(name_list3$var_name),new = as.character(name_list3$var_label),skip_absent = T)

selected_data_with_names <- selected_data %>%
  clean_names() %>%
  select(-page, -value, -download_links,-filter1, -filter2, -filter3)

## final dataset: for some reasons the respondent_sequence_number (id number of each individual) are duplicated, so simply use distinct() function to drop the duplicates. 
selected_data <- selected_data %>%
    distinct(respondent_sequence_number,.keep_all = T)
  

```
